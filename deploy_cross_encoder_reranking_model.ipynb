{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6238b-ceae-4f52-acf6-db32ff0c36f1",
   "metadata": {},
   "source": [
    "# Deploy a cross encoder model for re-ranking to Amazon SageMaker endpoint\n",
    "\n",
    "In this notebook, we demonstrate, how we can package and deploy a cross-encoder model for re-ranking.\n",
    "\n",
    "## Bi-Encoder vs. Cross-Encoder\n",
    "\n",
    "First, it is important to understand the difference between Bi- and Cross-Encoder.\n",
    "\n",
    "Bi-Encoders produce for a given sentence a sentence embedding. We pass to a BERT independently the sentences A and B, which result in the sentence embeddings u and v. These sentence embedding can then be compared using cosine similarity:\n",
    "\n",
    "![Bi vs Cross-encoder](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/Bi_vs_Cross-Encoder.png)\n",
    "\n",
    "In contrast, for a Cross-Encoder, we pass both sentences simultaneously to the Transformer network. It produces then an output value between 0 and 1 indicating the similarity of the input sentence pair:\n",
    "\n",
    "A Cross-Encoder _does not produce_ a sentence embedding. Also, we are not able to pass individual sentences to a Cross-Encoder.\n",
    "\n",
    "As detailed in [this](https://arxiv.org/abs/1908.10084) paper, Cross-Encoder achieve better performances than Bi-Encoders.\n",
    "\n",
    "However, for many application they are not pratical as they do not produce embeddings we could e.g. index or efficiently compare using cosine similarity.\n",
    "\n",
    "## Models\n",
    "\n",
    "- Cross-encoder model for re-ranking\n",
    "  - [sentence-transformers/ms-marco-MiniLM-L-12-v2](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2)\n",
    "\n",
    "## Inference script to handle both embedding and re-ranking\n",
    "\n",
    "Refer to [models/cross-encoders/ms-marco-MiniLM-L-12-v2/code/inference.py](./models/cross-encoders/ms-marco-MiniLM-L-12-v2/code/inference.py) script for implementation details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99786908-b4c9-4892-8941-deb01fdf0a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U sagemaker rich watermark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb393f-c374-41f4-9902-61ffc66db768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from rich import print\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1e588-e861-499f-bb6b-6ca3b1d20721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket_name = session.default_bucket()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "# Define sagemaker client object to invoke Sagemaker services\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "model_base_name = \"ms-marco-MiniLM-L-12-v2\"\n",
    "model_folder = Path(f\"./models/cross-encoders/{model_base_name}\").absolute().resolve()\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2367e0b-9178-4756-a3a1-087512ee599a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84138e8b-a245-4174-ba24-1dff210b14e0",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "- Compress model artifacts to `model.tar.gz`\n",
    "- Upload model to S3\n",
    "- Create Model object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef260d-42a8-4adc-8102-bc76de4c677a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_to_compress = [\n",
    "    \"pytorch_model.bin\",\n",
    "    \"config.json\",\n",
    "    \"vocab.txt\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"code\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bfd02-f6a2-4af2-9c2c-c4a709bdd601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change to model dir and run tar command\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")\n",
    "\n",
    "if not os.path.exists(str(model_archive_path)):\n",
    "    print(str(model_folder))\n",
    "    os.chdir(str(model_folder))\n",
    "    model_files = \" \".join(files_to_compress)\n",
    "    command = f\"tar -cf model.tar.gz --use-compress-program=pigz {model_files}\"\n",
    "    out = subprocess.run(command, shell=True, check=True)\n",
    "    if out.returncode != 0:\n",
    "        raise Exception(\"Failed to run compress files\")\n",
    "    else:\n",
    "        print(\"model.tar.gz created successfully!\")\n",
    "    os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb4872-ba59-47b4-b0df-ffb0ffa7e480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload model artifact to S3\n",
    "suffix = f\"/models/txt-embedding-models/cross-encoders/{model_base_name}\"\n",
    "upload_path_s3 = s3_path_join(f\"s3://{bucket_name}\", suffix)\n",
    "print(f\"Uploading the model to {upload_path_s3}\")\n",
    "model_data_url = S3Uploader.upload(\n",
    "    local_path=str(model_archive_path),\n",
    "    desired_s3_uri=upload_path_s3,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "print(f\"Model Data URL: {model_data_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762685e3-d007-4882-bcfd-55779bf4dec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "model_name = f\"{model_base_name}-{suffix}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280083f1-422c-4083-a35e-afa453bdf28f",
   "metadata": {},
   "source": [
    "Create HuggingFaceModel with model data and custom `inference.py` script\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1b1bd-8cc3-4d6b-9f96-54c0f89ec609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Creating model: {model_name}\")\n",
    "txt_embed_model = HuggingFaceModel(\n",
    "    model_data=model_data_url,\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    transformers_version=\"4.26.0\",\n",
    "    pytorch_version=\"1.13.1\",\n",
    "    sagemaker_session=session,\n",
    "    py_version=\"py39\",\n",
    "    name=model_name,\n",
    "    env={\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"10\"},\n",
    ")\n",
    "\n",
    "txt_embed_model.create(instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd8e3f-f2d7-4f40-958b-27ca95684e96",
   "metadata": {},
   "source": [
    "### Deploy Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824068ab-eebe-4f57-8f9b-db746dd03150",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to serverless endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b3835-8461-455e-baaa-82d20fe2f79c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Serverless endpoint\n",
    "\n",
    "endpoint_name = model_name\n",
    "endpoint_config_name = f\"{model_name}-epc\"\n",
    "\n",
    "# Memory In GiB\n",
    "memory = 2048\n",
    "max_concurrency = 10\n",
    "\n",
    "# Create endpoint config\n",
    "epc_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ServerlessConfig\": {\n",
    "                \"MemorySizeInMB\": memory,\n",
    "                \"MaxConcurrency\": max_concurrency,\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "status_code = epc_response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "epc_arn = epc_response[\"EndpointConfigArn\"]\n",
    "\n",
    "if status_code == 200:\n",
    "    print(f\"EPC : {endpoint_config_name} created\")\n",
    "    print(f\"Creating endpoint: {endpoint_name} ...\")\n",
    "    ep_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "    status_code = ep_response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "    print(f\"Endpoint: {endpoint_name}; Status Code: {status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb153a-d6fa-4d61-b1d4-32fec63d502f",
   "metadata": {},
   "source": [
    "### Wait for endpoint to be `InService` state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399f28f-779b-4636-86a5-177c6c625904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")\n",
    "\n",
    "# Get the waiter object\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "# Apply the waiter on the endpoint\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "# Get endpoint status using describe endpoint\n",
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e07b19-5530-4124-be5d-9d3da8e79291",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to real-time endpoint (Optional)\n",
    "\n",
    "Uncomment below code to deploy this to a real-time endpoint instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d5503-4e56-4a93-8fa0-821323349151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = model_name\n",
    "\n",
    "# predictor = txt_embed_model.deploy(\n",
    "#     instance_type=instance_type,\n",
    "#     initial_instance_count=instance_count,\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     serializer=JSONSerializer(),\n",
    "#     deserializer=JSONDeserializer(),\n",
    "#     wait=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028b422-8ce0-460f-bc4b-081909d294e1",
   "metadata": {},
   "source": [
    "### Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b184736-fe51-4d8b-b095-e8d5d8e189aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "input_data = {\n",
    "    \"sentence\": \"I love Berlin\",\n",
    "    \"candidates\": [\"I love Paris\", \"I love Dusseldorf\", \"I love Hannover\"],\n",
    "}\n",
    "\n",
    "rankings = predictor.predict(input_data)\n",
    "\n",
    "print(rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c0fb5-96db-4b47-9dd5-66bfd9c822d6",
   "metadata": {},
   "source": [
    "### Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0dedf-fbbc-48a1-ae91-c64cdf59a588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictor.delete_model()\n",
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
