{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6238b-ceae-4f52-acf6-db32ff0c36f1",
   "metadata": {},
   "source": [
    "# Deploy `BAAI/bge-large-en` Text Embedding Model (1024 Dimension) to Amazon SageMaker\n",
    "\n",
    "In this notebook, we demonstrate, how we can package and deploy `BAAI/bge-large-en` embedding model with 1024 dimensions.\n",
    "\n",
    "`bge` is short for BAAI general embedding.\n",
    "\n",
    "*NOTE*: If you need to search the long relevant passages to a short query (s2p retrieval task), you need to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, no instruction need to be added to passages.\n",
    "\n",
    "\n",
    "**NOTE:** bge model sizes and dimension\n",
    "- `BAAI/bge-base-en`: **~438MB** (Dimensions: 768)\n",
    "- `BAAI/bge-large-en`: **~1.34GB** (Dimensions: 1024)\n",
    "\n",
    "## References\n",
    "https://github.com/FlagOpen/FlagEmbedding\n",
    "\n",
    "## Inference script to handle both embedding and re-ranking\n",
    "\n",
    "Refer to [./models/bi-encoders/bge-large-en/code/inference.py](./models/bi-encoders/gte-base/code/inference.py) for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99786908-b4c9-4892-8941-deb01fdf0a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq sagemaker rich watermark ipywidgets\n",
    "\n",
    "%load_ext rich\n",
    "%load_ext watermark\n",
    "%watermark -p sagemaker,ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb393f-c374-41f4-9902-61ffc66db768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from huggingface_hub import snapshot_download\n",
    "from rich import print\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "from sagemaker.session import Session\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1e588-e861-499f-bb6b-6ca3b1d20721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "session = Session()\n",
    "bucket_name = session.default_bucket()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "\n",
    "HF_MODEL_ID = \"BAAI/bge-large-en\"\n",
    "model_base_name = HF_MODEL_ID.split(\"/\")[-1]\n",
    "model_folder = Path(f\"./models/bi-encoders/{model_base_name}\").absolute().resolve()\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")\n",
    "\n",
    "print(f\"Region: [i]{region}[/i]\")\n",
    "print(f\"bucket name: {bucket_name}\")\n",
    "print(model_folder)\n",
    "print(model_archive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2367e0b-9178-4756-a3a1-087512ee599a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bin = model_folder.joinpath(\"pytorch_model.bin\")\n",
    "\n",
    "if not model_bin.exists():\n",
    "    print(f\"Downloading model ...\")\n",
    "    snapshot_download(\n",
    "        repo_id=HF_MODEL_ID,\n",
    "        local_dir=model_folder,\n",
    "        local_dir_use_symlinks=False,\n",
    "        allow_patterns=[\"1_Pooling\", \"*.txt\", \"*.json\", \"*.bin\", \"*.safetensors\"],\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84138e8b-a245-4174-ba24-1dff210b14e0",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "- Compress model artifacts to `model.tar.gz`\n",
    "- Upload model to S3\n",
    "- Create Model object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef260d-42a8-4adc-8102-bc76de4c677a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_to_compress = [\n",
    "    \"pytorch_model.bin\",\n",
    "    \"config.json\",\n",
    "    \"modules.json\",\n",
    "    \"vocab.txt\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"config_sentence_transformers.json\",\n",
    "    \"sentence_bert_config.json\",\n",
    "    \"1_Pooling\",\n",
    "    \"code\",\n",
    "]\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84a7ca-5f58-4b4a-8f31-8ef35c10685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(model_archive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a186352-9989-48a2-9998-8d23adf59615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to model dir and run tar command\n",
    "# current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "print(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bfd02-f6a2-4af2-9c2c-c4a709bdd601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(current_dir)\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")\n",
    "\n",
    "if model_archive_path.exists():\n",
    "    model_archive_path.unlink()\n",
    "\n",
    "if not model_archive_path.exists():\n",
    "    print(str(model_folder))\n",
    "    # change to model dir and run tar command\n",
    "    os.chdir(str(model_folder))\n",
    "    model_files = \" \".join(files_to_compress)\n",
    "    command = f\"tar -cvzf model.tar.gz --use-compress-program=pigz --exclude='**/.ipynb_checkpoints' --exclude='.DS_Store' {model_files}\"\n",
    "    result = subprocess.run(command, shell=True, check=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"tar created successfully at {model_archive_path}!\")\n",
    "    else:\n",
    "        os.chdir(current_dir)\n",
    "        print(result.stderr)\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842ab79-6ad6-41a2-a27d-20da08cfecec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify contents of the model archive.\n",
    "!tar tvf $model_archive_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb4872-ba59-47b4-b0df-ffb0ffa7e480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload model artifact to S3\n",
    "suffix = f\"/models/txt-embedding-models/{model_base_name}\"\n",
    "upload_path_s3 = s3_path_join(f\"s3://{bucket_name}\", suffix)\n",
    "print(f\"Uploading the model to {upload_path_s3} ...\")\n",
    "\n",
    "model_data_url = S3Uploader.upload(\n",
    "    local_path=str(model_archive_path),\n",
    "    desired_s3_uri=upload_path_s3,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "print(f\"Model Data URL: {model_data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280083f1-422c-4083-a35e-afa453bdf28f",
   "metadata": {},
   "source": [
    "Create HuggingFaceModel with model data and custom `inference.py` script\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1b1bd-8cc3-4d6b-9f96-54c0f89ec609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "model_name = f\"{model_base_name}-{suffix}\"\n",
    "\n",
    "print(f\"Creating model: {model_name}\")\n",
    "\n",
    "txt_embed_model = HuggingFaceModel(\n",
    "    model_data=model_data_url,\n",
    "    role=role,\n",
    "    transformers_version=\"4.26.0\",\n",
    "    pytorch_version=\"1.13.1\",\n",
    "    sagemaker_session=session,\n",
    "    py_version=\"py39\",\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd8e3f-f2d7-4f40-958b-27ca95684e96",
   "metadata": {},
   "source": [
    "### Deploy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824068ab-eebe-4f57-8f9b-db746dd03150",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to serverless endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b3835-8461-455e-baaa-82d20fe2f79c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "# Memory In GiB\n",
    "memory = 4096\n",
    "max_concurrency = 10\n",
    "endpoint_name = model_name\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=memory, max_concurrency=max_concurrency\n",
    ")\n",
    "\n",
    "print(f\"Creating endpoint: [b]{endpoint_name}[/b] ...\")\n",
    "\n",
    "# Returns a HuggingFacePredictor\n",
    "predictor = txt_embed_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serverless_inference_config=serverless_config,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb153a-d6fa-4d61-b1d4-32fec63d502f",
   "metadata": {},
   "source": [
    "### Wait for endpoint to be `InService` state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399f28f-779b-4636-86a5-177c6c625904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")\n",
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")\n",
    "\n",
    "# Get the waiter object\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "# Apply the waiter on the endpoint\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "# Get endpoint status using describe endpoint\n",
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e07b19-5530-4124-be5d-9d3da8e79291",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to real-time endpoint (Optional)\n",
    "\n",
    "Uncomment below code to deploy this to a real-time endpoint instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d5503-4e56-4a93-8fa0-821323349151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = model_name\n",
    "\n",
    "# predictor = txt_embed_model.deploy(\n",
    "#     instance_type=instance_type,\n",
    "#     initial_instance_count=instance_count,\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     serializer=JSONSerializer(),\n",
    "#     deserializer=JSONDeserializer(),\n",
    "#     wait=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028b422-8ce0-460f-bc4b-081909d294e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict\n",
    "\n",
    "```python\n",
    "def generate_embeddings(texts, model, tokenizer, normalize=True):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): List of texts to calculate embeddings for.\n",
    "        model (AutoModel): Pre-trained model.\n",
    "        tokenizer (AutoTokenizer): Tokenizer corresponding to the pre-trained model.\n",
    "        normalize (bool, optional): Whether to normalize the embeddings. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor containing the embeddings for the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Tokenize the texts\n",
    "    encoded_input = tokenizer(\n",
    "        texts, max_length=512, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    encoded_input = encoded_input.to(device)\n",
    "\n",
    "    # Get the embeddings for the texts\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "        # Perform pooling. In this case, cls pooling.\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "\n",
    "\n",
    "    # Normalize embeddings if required\n",
    "    if normalize:\n",
    "        sentence_embeddings = F.normalize(text_embeddings, p=2, dim=1)\n",
    "\n",
    "    # convert to numpy array\n",
    "    sentence_embeddings = sentence_embeddings.cpu().numpy()\n",
    "    ret_value = sentence_embeddings.tolist()\n",
    "\n",
    "    return ret_value\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    logger.info(\"model_fn\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    embeddings_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    embeddings_model = AutoModel.from_pretrained(model_dir)\n",
    "    embeddings_model.eval()\n",
    "    embeddings_model.to(device)\n",
    "\n",
    "    model = {\n",
    "        \"embeddings_model\": embeddings_model,\n",
    "        \"embeddings_tokenizer\": embeddings_tokenizer\n",
    "    }\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_fn(texts, model):\n",
    "    logger.info(\"predict_fn\")\n",
    "\n",
    "    embeddings_model = model[\"embeddings_model\"]\n",
    "    embeddings_tokenizer = model[\"embeddings_tokenizer\"]\n",
    "\n",
    "    ret_value = generate_embeddings(texts, embeddings_model, embeddings_tokenizer)\n",
    "\n",
    "    return ret_value\n",
    "```\n",
    "\n",
    "Ref: <https://huggingface.co/thenlper/gte-base>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b716c-f3f7-4c51-99e7-086e5d628c32",
   "metadata": {},
   "source": [
    "#### Uncomment below code block if you are invoking an existing endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8671b-ac4a-4ea7-a4d2-18b09d7861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = \"gte-base-cc0cc-03Aug2023\"\n",
    "# predictor = Predictor(\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     sagemaker_session=session,\n",
    "#     serializer=JSONSerializer(),\n",
    "#     deserializer=JSONDeserializer()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b184736-fe51-4d8b-b095-e8d5d8e189aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\"That is a happy person\", \"That is a very happy person\"]\n",
    "\n",
    "embeddings = predictor.predict(sentences)\n",
    "\n",
    "print(f\"Embedding dimensions: {len(embeddings[0])}\")\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c0fb5-96db-4b47-9dd5-66bfd9c822d6",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0dedf-fbbc-48a1-ae91-c64cdf59a588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
