{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6238b-ceae-4f52-acf6-db32ff0c36f1",
   "metadata": {},
   "source": [
    "# Deploy `thenlper/gte-base` Text Embedding Model (768 Dimension) to Amazon SageMaker\n",
    "\n",
    "In this notebook, we demonstrate, how we can package and deploy `thenlper/gte-base` embedding model with 768 dimensions.\n",
    "\n",
    "**Gegeral Text Embeddings (GTE) model**\n",
    "\n",
    "The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including GTE-large, GTE-base, and GTE-small. The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including information retrieval, semantic textual similarity, text reranking, etc.\n",
    "\n",
    "**NOTE:** gte model sizes are comparitively smaller than other top performing embedding models\n",
    "\n",
    "- `thenlper/gte-small`: **~67MB**\n",
    "- `thenlper/gte-base`: **~220MB**\n",
    "- `thenlper/gte-large`: **~670MB**\n",
    "\n",
    "## Papers\n",
    "\n",
    "N/A as of 03/08/2023\n",
    "\n",
    "## Models\n",
    "\n",
    "- [`thenlper/gte-small`](https://hf.co/thenlper/gte-small)\n",
    "- [`thenlper/gte-base`](https://hf.co/thenlper/gte-base)\n",
    "- [`thenlper/gte-large`](https://hf.co/thenlper/gte-large)\n",
    "\n",
    "## Inference script to handle both embedding and re-ranking\n",
    "\n",
    "Refer to [./models/bi-encoders/gte-base/code/inference.py](./models/bi-encoders/gte-base/code/inference.py) for implementation details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99786908-b4c9-4892-8941-deb01fdf0a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U sagemaker rich watermark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb393f-c374-41f4-9902-61ffc66db768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from rich import print\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.session import Session\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "from sagemaker.serverless import ServerlessInferenceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1e588-e861-499f-bb6b-6ca3b1d20721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = Session()\n",
    "bucket_name = session.default_bucket()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "# Define sagemaker client object to invoke Sagemaker services\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "HF_MODEL_ID = \"thenlper/gte-base\"\n",
    "model_base_name = HF_MODEL_ID.split(\"/\")[-1]\n",
    "model_folder = Path(f\"./models/bi-encoders/{model_base_name}\").absolute().resolve()\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "print(model_folder)\n",
    "print(model_archive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2367e0b-9178-4756-a3a1-087512ee599a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not model_folder.exists():\n",
    "    print(f\"Downloading model ...\")\n",
    "    snapshot_download(repo_id=HF_MODEL_ID, local_dir=model_folder, local_dir_use_symlinks=False)\n",
    "else:\n",
    "    print(f\"Model already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84138e8b-a245-4174-ba24-1dff210b14e0",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "- Compress model artifacts to `model.tar.gz`\n",
    "- Upload model to S3\n",
    "- Create Model object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef260d-42a8-4adc-8102-bc76de4c677a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_to_compress = [\n",
    "    \"pytorch_model.bin\",\n",
    "    \"config.json\",\n",
    "    \"vocab.txt\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"sentence_bert_config.json\",\n",
    "    \"1_Pooling\",\n",
    "    \"code\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a186352-9989-48a2-9998-8d23adf59615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to model dir and run tar command\n",
    "print(current_dir)\n",
    "print(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bfd02-f6a2-4af2-9c2c-c4a709bdd601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")\n",
    "\n",
    "if model_archive_path.exists():\n",
    "    model_archive_path.unlink()\n",
    "\n",
    "if not model_archive_path.exists():\n",
    "    print(str(model_folder))\n",
    "    os.chdir(str(model_folder))\n",
    "    model_files = \" \".join(files_to_compress)\n",
    "    command = f\"tar -cf model.tar.gz --use-compress-program=pigz {model_files}\"\n",
    "    out = subprocess.run(command, shell=True, check=True)\n",
    "    if out.returncode != 0:\n",
    "        raise Exception(\"Failed to compress model files\")\n",
    "    else:\n",
    "        print(\"Model files compressed successfully\")\n",
    "    os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb4872-ba59-47b4-b0df-ffb0ffa7e480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload model artifact to S3\n",
    "suffix = f\"/models/txt-embedding-models/{model_base_name}\"\n",
    "upload_path_s3 = s3_path_join(f\"s3://{bucket_name}\", suffix)\n",
    "print(f\"Uploading the model to {upload_path_s3}\")\n",
    "model_data_url = S3Uploader.upload(\n",
    "    local_path=str(model_archive_path),\n",
    "    desired_s3_uri=upload_path_s3,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "print(f\"Model Data URL: {model_data_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762685e3-d007-4882-bcfd-55779bf4dec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "model_name = f\"{model_base_name}-{suffix}\"\n",
    "print(f\"Model Name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280083f1-422c-4083-a35e-afa453bdf28f",
   "metadata": {},
   "source": [
    "Create HuggingFaceModel with model data and custom `inference.py` script\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1b1bd-8cc3-4d6b-9f96-54c0f89ec609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Creating model: {model_name}\")\n",
    "txt_embed_model = HuggingFaceModel(\n",
    "    model_data=model_data_url,\n",
    "    role=role,\n",
    "    transformers_version=\"4.26.0\",\n",
    "    pytorch_version=\"1.13.1\",\n",
    "    sagemaker_session=session,\n",
    "    py_version=\"py39\",\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd8e3f-f2d7-4f40-958b-27ca95684e96",
   "metadata": {},
   "source": [
    "### Deploy Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824068ab-eebe-4f57-8f9b-db746dd03150",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to serverless endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b3835-8461-455e-baaa-82d20fe2f79c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "# Memory In GiB\n",
    "memory = 2048\n",
    "max_concurrency = 10\n",
    "endpoint_name = model_name\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=memory, max_concurrency=max_concurrency\n",
    ")\n",
    "\n",
    "print(f\"Creating endpoint: [b]{endpoint_name}[/b] ...\")\n",
    "\n",
    "# Returns a HuggingFacePredictor\n",
    "predictor = txt_embed_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serverless_inference_config=serverless_config,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb153a-d6fa-4d61-b1d4-32fec63d502f",
   "metadata": {},
   "source": [
    "### Wait for endpoint to be `InService` state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399f28f-779b-4636-86a5-177c6c625904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")\n",
    "\n",
    "# Get the waiter object\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "# Apply the waiter on the endpoint\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "# Get endpoint status using describe endpoint\n",
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e07b19-5530-4124-be5d-9d3da8e79291",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to real-time endpoint (Optional)\n",
    "\n",
    "Uncomment below code to deploy this to a real-time endpoint instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d5503-4e56-4a93-8fa0-821323349151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = model_name\n",
    "\n",
    "# predictor = txt_embed_model.deploy(\n",
    "#     instance_type=instance_type,\n",
    "#     initial_instance_count=instance_count,\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     serializer=JSONSerializer(),\n",
    "#     deserializer=JSONDeserializer(),\n",
    "#     wait=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028b422-8ce0-460f-bc4b-081909d294e1",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "Do I need to add the prefix \"query: \" and \"passage: \" to input texts?\n",
    "\n",
    "Yes, this is how the model is trained, otherwise you will see a performance degradation.\n",
    "\n",
    "Here are some rules of thumb:\n",
    "\n",
    "- Use _\"query: \"_ and _\"passage: \"_ correspondingly for **asymmetric tasks** such as passage retrieval in open QA, ad-hoc information retrieval.\n",
    "- Use **\"query: \"** prefix for **symmetric tasks** such as semantic similarity, paraphrase retrieval.\n",
    "- Use **\"query: \"** prefix if you want to use embeddings as features, such as linear probing classification, clustering.\n",
    "\n",
    "Ref: <https://huggingface.co/intfloat/e5-base-v2#faq>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b716c-f3f7-4c51-99e7-086e5d628c32",
   "metadata": {},
   "source": [
    "#### Uncomment below code block if you are invoking an existing endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8671b-ac4a-4ea7-a4d2-18b09d7861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = \"gte-base-cc0cc-03Aug2023\"\n",
    "# predictor = Predictor(\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     sagemaker_session=session,\n",
    "#     serializer=JSONSerializer(),\n",
    "#     deserializer=JSONDeserializer()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b184736-fe51-4d8b-b095-e8d5d8e189aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\"That is a happy person\", \"That is a very happy person\"]\n",
    "\n",
    "embeddings = predictor.predict(sentences)\n",
    "\n",
    "print(f\"Embedding dimensions: {len(embeddings[0])}\")\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c0fb5-96db-4b47-9dd5-66bfd9c822d6",
   "metadata": {},
   "source": [
    "### Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0dedf-fbbc-48a1-ae91-c64cdf59a588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictor.delete_model()\n",
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
