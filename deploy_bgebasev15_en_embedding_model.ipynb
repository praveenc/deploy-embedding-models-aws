{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6238b-ceae-4f52-acf6-db32ff0c36f1",
   "metadata": {},
   "source": [
    "# Deploy `BAAI/bge-base-en-v1.5` Text Embedding Model (728 Dimension) to Amazon SageMaker\n",
    "\n",
    "In this notebook, we demonstrate to package and deploy `BAAI/bge-base-en-v1.5` text embedding model with **768** dimensions.\n",
    "\n",
    "`bge` is short for BAAI general embedding.\n",
    "\n",
    "*NOTE*: If you need to search the long relevant passages to a short query (s2p retrieval task), you need to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, no instruction need to be added to passages.\n",
    "\n",
    "Refer to **Model Card:** <https://huggingface.co/BAAI/bge-base-en-v1.5#using-huggingface-transformers> for more details.\n",
    "\n",
    "**NOTE:** bge model sizes and dimension\n",
    "- `BAAI/bge-base-en-v1.5`: **~438MB** (Dimensions: 768)\n",
    "- `BAAI/bge-large-en-v1.5`: **~1.34GB** (Dimensions: 1024)\n",
    "\n",
    "## References\n",
    "- <<https://huggingface.co/BAAI/bge-base-en-v1.5>\n",
    "- <https://github.com/FlagOpen/FlagEmbedding>\n",
    "\n",
    "## Inference script\n",
    "\n",
    "Refer to [inference.py](./models/bi-encoders/bge-base-en-v15/code/inference.py) for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99786908-b4c9-4892-8941-deb01fdf0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq boto3 sagemaker rich watermark ipywidgets\n",
    "# %load_ext rich\n",
    "# %load_ext watermark\n",
    "# %watermark -p boto3,sagemaker,ipywidgets,transformers\n",
    "# %watermark -m -t -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb393f-c374-41f4-9902-61ffc66db768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from rich import print\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sys.path.append(\"./utils\")\n",
    "from utils import sm_utils, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1e588-e861-499f-bb6b-6ca3b1d20721",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "bucket = session.default_bucket()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "\n",
    "HF_MODEL_ID = \"BAAI/bge-base-en-v1.5\"\n",
    "model_base_name = HF_MODEL_ID.split(\"/\")[-1].replace(\".\", \"\")\n",
    "model_folder = Path(f\"./models/bi-encoders/{model_base_name}\")\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")\n",
    "s3_baseuri = s3_path_join(f\"s3://{bucket}/models\", f\"txt-embedding-models/{model_base_name}\")\n",
    "\n",
    "print(f\"Region: [i]{region}[/i]\")\n",
    "print(f\"bucket name: {s3_baseuri}\")\n",
    "print(f\"Model dir: {model_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2367e0b-9178-4756-a3a1-087512ee599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = model_folder.joinpath(\"pytorch_model.bin\")\n",
    "\n",
    "if not model_bin.exists():\n",
    "    print(\"Downloading model ...\")\n",
    "    snapshot_download(\n",
    "        repo_id=HF_MODEL_ID,\n",
    "        local_dir=str(model_folder),\n",
    "        local_dir_use_symlinks=False,\n",
    "        allow_patterns=[\"1_Pooling\", \"*.txt\", \"*.json\", \"*.bin\"],\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model already downloaded. {model_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84138e8b-a245-4174-ba24-1dff210b14e0",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "- Compress model artifacts to `model.tar.gz`\n",
    "- Upload model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db6179-b99b-45d1-a911-9d688ce177f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.clear_ipynb_dirs(model_folder)  # remove any .ipynb_checkpoints, __pycache__\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")\n",
    "if model_archive_path.exists():\n",
    "    print(f\"Deleting existing model: {model_archive_path}\")\n",
    "    model_archive_path.unlink(missing_ok=True)\n",
    "\n",
    "print(f\"Creating archive with base_dir={model_folder}\")\n",
    "model_archive_path = shutil.make_archive(\n",
    "    format=\"gztar\",  # tar.gz format\n",
    "    base_name=model_folder.name,  # will create model.tar.gz\n",
    "    root_dir=model_folder,  # dir to chdir into before archiving\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842ab79-6ad6-41a2-a27d-20da08cfecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify contents of the model archive.\n",
    "# !tar tvf $model_archive_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067f44d-488d-4df5-a4f4-0e1615f0ed25",
   "metadata": {},
   "source": [
    "#### Upload archive to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb4872-ba59-47b4-b0df-ffb0ffa7e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Uploading model from {model_archive_path} to \\n{s3_baseuri} ...\")\n",
    "model_data_url = S3Uploader.upload(\n",
    "    local_path=str(model_archive_path),\n",
    "    desired_s3_uri=s3_baseuri,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "print(f\"Model Data URL: {model_data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280083f1-422c-4083-a35e-afa453bdf28f",
   "metadata": {},
   "source": [
    "Create HuggingFaceModel with model data and custom `inference.py` script\n",
    "\n",
    "<https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933c5e92-8743-4c24-a91d-3b6e5edd833d",
   "metadata": {},
   "source": [
    "### Deploy to real-time endpoint\n",
    "\n",
    "Serveless endpoint can only host models with total image size + model size <= 10GB\n",
    "\n",
    "The HuggingFace Transformers Model container + BGE Basemodel exceeds 10GB So, we deploy to real-time endpoint here.\n",
    "\n",
    "Helper functions to create and deploy huggingface model is under [utils (sm_utils)](./utils/sm_utils.py) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1b1bd-8cc3-4d6b-9f96-54c0f89ec609",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.c5.4xlarge\"\n",
    "suffix = utils.get_suffix()  # returns a uui4-datetime formatted string\n",
    "model_name = f\"{model_base_name}-{suffix}\"\n",
    "env = {\"HF_TASK\": \"feature-extraction\"}  # HF_TASK is required for HF models\n",
    "\n",
    "# function to create and deploy model to real-time endpoint\n",
    "predictor = sm_utils.create_deploy_huggingface_model(\n",
    "    model_name=model_name,\n",
    "    model_s3uri=model_data_url,\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb153a-d6fa-4d61-b1d4-32fec63d502f",
   "metadata": {},
   "source": [
    "### Wait for endpoint to come online  (`InService`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399f28f-779b-4636-86a5-177c6c625904",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_utils.get_endpoint_status(endpoint_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028b422-8ce0-460f-bc4b-081909d294e1",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Refer to [models/bi-encoders/bge-base-en-v15/code/inference.py](./models/bi-encoders/bge-base-en-v15/code/inference.py) for complete implementation.\n",
    "\n",
    "**Model Card:** <https://huggingface.co/BAAI/bge-base-en-v1.5#using-huggingface-transformers>\n",
    "\n",
    "```python\n",
    "def generate_embeddings(texts, model, tokenizer, normalize=True):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): List of texts to calculate embeddings for.\n",
    "        model (AutoModel): Pre-trained model.\n",
    "        tokenizer (AutoTokenizer): Tokenizer corresponding to the pre-trained model.\n",
    "        normalize (bool, optional): Whether to normalize the embeddings. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor containing the embeddings for the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Tokenize the texts\n",
    "    encoded_input = tokenizer(\n",
    "        texts, max_length=512, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    encoded_input = encoded_input.to(device)\n",
    "\n",
    "    # Get the embeddings for the texts\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "        # Perform pooling. In this case, cls pooling.\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "\n",
    "\n",
    "    # Normalize embeddings if required\n",
    "    if normalize:\n",
    "        sentence_embeddings = F.normalize(text_embeddings, p=2, dim=1)\n",
    "\n",
    "    # convert to numpy array\n",
    "    sentence_embeddings = sentence_embeddings.cpu().numpy()\n",
    "    ret_value = sentence_embeddings.tolist()\n",
    "\n",
    "    return ret_value\n",
    "\n",
    "...\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b716c-f3f7-4c51-99e7-086e5d628c32",
   "metadata": {},
   "source": [
    "### Invoke Endpoint\n",
    "\n",
    "Before we invoke we attach `JSONSerializer` and `JSONDeserializer` to the predictor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8671b-ac4a-4ea7-a4d2-18b09d7861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b184736-fe51-4d8b-b095-e8d5d8e189aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"That is a happy person\", \"That is a very happy person\"]\n",
    "\n",
    "embeddings = predictor.predict(sentences)\n",
    "print(f\"Embedding dimensions: {len(embeddings)}\")  # returns 2 embeddings\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c0fb5-96db-4b47-9dd5-66bfd9c822d6",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete resources after use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0dedf-fbbc-48a1-ae91-c64cdf59a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Deleting model and endpoint: {model_name}\")\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
