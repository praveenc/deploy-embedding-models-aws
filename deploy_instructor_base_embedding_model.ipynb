{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6238b-ceae-4f52-acf6-db32ff0c36f1",
   "metadata": {},
   "source": [
    "# Deploy hkunlp/instructor embedding models to Amazon SageMaker\n",
    "\n",
    "In this notebook, we demonstrate packaging/deploying hkunlp/instructor-\\* embedding models with 768 dimensions to Amazon SageMaker.\n",
    "\n",
    "an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructorüë®‚Äç achieves sota on 70 diverse embedding tasks! The model is easy to use with our customized sentence-transformer library.\n",
    "\n",
    "## Papers\n",
    "\n",
    "- https://arxiv.org/abs/2212.09741\n",
    "- https://instructor-embedding.github.io/\n",
    "- https://huggingface.co/papers/2212.09741\n",
    "\n",
    "## Models\n",
    "\n",
    "- instructor-base\n",
    "  - [hkunlp/instructor-base](https://huggingface.co/hkunlp/instructor-base)\n",
    "- instructor-large\n",
    "  - [hkunlp/instructor-large](https://huggingface.co/hkunlp/instructor-large)\n",
    "\n",
    "## Inference script to handle both embedding and re-ranking\n",
    "\n",
    "Refer to [./models/bi-encoders/instructor-base/code/inference.py](./models/bi-encoders/instructor-base/code/inference.py) for implementation details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99786908-b4c9-4892-8941-deb01fdf0a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U sagemaker rich watermark InstructorEmbedding --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb393f-c374-41f4-9902-61ffc66db768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from huggingface_hub import snapshot_download\n",
    "from rich import print\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader, s3_path_join\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1e588-e861-499f-bb6b-6ca3b1d20721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket_name = session.default_bucket()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "# Define sagemaker client object to invoke Sagemaker services\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "HF_MODEL_ID = \"hkunlp/instructor-base\"\n",
    "model_base_name = HF_MODEL_ID.split(\"/\")[-1]\n",
    "model_folder = Path(f\"./models/bi-encoders/{model_base_name}\").absolute().resolve()\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")\n",
    "code_archive_path = model_folder.joinpath(\"sourcedir.tar.gz\")\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2367e0b-9178-4756-a3a1-087512ee599a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not model_folder.exists():\n",
    "    snapshot_download(HF_MODEL_ID, local_dir=str(model_folder), local_dir_use_symlinks=False)\n",
    "else:\n",
    "    print(f\"Model {HF_MODEL_ID} exists at: {str(model_folder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84138e8b-a245-4174-ba24-1dff210b14e0",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "- Compress model artifacts to `model.tar.gz`\n",
    "- Upload model to S3\n",
    "- Create Model object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef260d-42a8-4adc-8102-bc76de4c677a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_files_to_compress = [\n",
    "    \"pytorch_model.bin\",\n",
    "    \"spiece.model\",\n",
    "    \"config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"modules.json\",\n",
    "    \"sentence_bert_config.json\",\n",
    "    \"config_sentence_transformers.json\",\n",
    "    \"1_Pooling\",\n",
    "    \"2_Dense\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bfd02-f6a2-4af2-9c2c-c4a709bdd601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change to model dir and run tar command\n",
    "print(current_dir)\n",
    "\n",
    "model_archive_path = model_folder.joinpath(\"model.tar.gz\")\n",
    "if model_archive_path.exists():\n",
    "    model_archive_path.unlink()\n",
    "\n",
    "if not os.path.exists(str(model_archive_path)):\n",
    "    print(str(model_folder))\n",
    "    os.chdir(str(model_folder))\n",
    "    model_files = \" \".join(model_files_to_compress)\n",
    "    print(f\"Compressing model files to model.tar.gz...\")\n",
    "    command = f\"tar -cf model.tar.gz --use-compress-program=pigz {model_files}\"\n",
    "    result = subprocess.run(command, shell=True, check=True)\n",
    "    if result.returncode != 0:\n",
    "        raise Exception(f\"Failed to compress model files: {result.stderr}\")\n",
    "    else:\n",
    "        print(\"Successfully compressed model files\")\n",
    "    os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb4872-ba59-47b4-b0df-ffb0ffa7e480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload model artifact to S3\n",
    "s3_suffix = f\"models/txt-embedding-models/{model_base_name}\"\n",
    "upload_path_s3 = s3_path_join(f\"s3://{bucket_name}\", s3_suffix)\n",
    "print(f\"Uploading model ...\")\n",
    "model_data_url = S3Uploader.upload(\n",
    "    local_path=str(model_archive_path),\n",
    "    desired_s3_uri=upload_path_s3,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "print(f\"Model Data URL: {model_data_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762685e3-d007-4882-bcfd-55779bf4dec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "model_name = f\"{model_base_name}-{suffix}\"\n",
    "print(f\"Model name: [b]{model_name}[/b]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280083f1-422c-4083-a35e-afa453bdf28f",
   "metadata": {},
   "source": [
    "Create HuggingFaceModel with model data and custom `inference.py` script\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model\n",
    "\n",
    "**NOTE:** If you specify `entry_point=` parameter in `HuggingFaceModel` then model artifacts will be uploaded to root of default Sagemaker S3 Bucket. If ignored, model will be created using artifacts from `model_data_url` in S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600ef6e-764b-408e-91e6-6395cfe0fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1b1bd-8cc3-4d6b-9f96-54c0f89ec609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Creating model: {model_name}\")\n",
    "\n",
    "txt_embed_model = HuggingFaceModel(\n",
    "    model_data=model_data_url,\n",
    "    role=role,\n",
    "    transformers_version=\"4.26.0\",\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    pytorch_version=\"1.13.1\",\n",
    "    sagemaker_session=session,\n",
    "    py_version=\"py39\",\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd8e3f-f2d7-4f40-958b-27ca95684e96",
   "metadata": {},
   "source": [
    "### Deploy Model to Serverless endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127fd4c-ff39-4afc-b322-b014157fc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "# Memory In GiB\n",
    "memory = 4096\n",
    "max_concurrency = 10\n",
    "endpoint_name = model_name\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=memory, max_concurrency=max_concurrency\n",
    ")\n",
    "\n",
    "print(f\"Creating endpoint: [b]{endpoint_name}[/b] ...\")\n",
    "\n",
    "# Returns a HuggingFacePredictor\n",
    "predictor = txt_embed_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serverless_inference_config=serverless_config,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    wait=False,\n",
    "    env={\n",
    "        \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb153a-d6fa-4d61-b1d4-32fec63d502f",
   "metadata": {},
   "source": [
    "### Wait for endpoint to be `InService` state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399f28f-779b-4636-86a5-177c6c625904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")\n",
    "\n",
    "# Get the waiter object\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "# Apply the waiter on the endpoint\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "# Get endpoint status using describe endpoint\n",
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint [b]{endpoint_name}[/b] Status: [i]{status}[/i]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e07b19-5530-4124-be5d-9d3da8e79291",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to real-time endpoint (Optional)\n",
    "\n",
    "Uncomment below code to deploy this to a real-time endpoint instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d5503-4e56-4a93-8fa0-821323349151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = model_name\n",
    "\n",
    "# predictor = txt_embed_model.deploy(\n",
    "#     instance_type=instance_type,\n",
    "#     initial_instance_count=instance_count,\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     serializer=JSONSerializer(),\n",
    "#     deserializer=JSONDeserializer(),\n",
    "#     wait=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028b422-8ce0-460f-bc4b-081909d294e1",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "Inputs must contain \"instruction\" based on the embedding task.\n",
    "\n",
    "For e.g., for information retrieval\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query  = [['Represent the question for retrieving documents: ','Which regions is Amazon SageMaker available?']]\n",
    "corpus = [['Represent the document for retrieval: ','Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. 1\tFor a list of the supported Amazon SageMaker AWS Regions, please visit the¬†AWS Regional Services page. Also, for more information, see¬†Regional endpoints¬†in the AWS general reference guide.'],\n",
    "          ['Represent the document for retrieval: ',\"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loans√¢‚Ç¨‚Äùand some scholars have argued that the theory's use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession\"],\n",
    "          ['Represent the document for retrieval: ','Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.']]\n",
    "\n",
    "query_embeddings = model.encode(query)\n",
    "corpus_embeddings = model.encode(corpus)\n",
    "\n",
    "similarities = cosine_similarity(query_embeddings,corpus_embeddings)\n",
    "\n",
    "retrieved_doc_id = np.argmax(similarities)\n",
    "\n",
    "print(retrieved_doc_id)\n",
    "```\n",
    "\n",
    "Ref: <https://huggingface.co/hkunlp/instructor-base>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d21c2-7357-4c2b-b7c9-d5bed0621817",
   "metadata": {},
   "source": [
    "### Uncomment below for existing endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d6071-f737-4125-b13b-7ff71dd6a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name =\"instructor-base-77ffb-02Aug2023\"\n",
    "\n",
    "# predictor = sagemaker.huggingface.HuggingFacePredictor(\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     serializer=JSONSerializer(),\n",
    "#     deserializer=JSONDeserializer(),\n",
    "#     sagemaker_session=session\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b184736-fe51-4d8b-b095-e8d5d8e189aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Invoking endpoint: {endpoint_name}\")\n",
    "\n",
    "sentences = [\n",
    "    [\n",
    "        \"Represent the question for retrieving documents: \",\n",
    "        \"This is an example question sentence\",\n",
    "    ],\n",
    "    [\"Represent the document for retrieval: \", \"This is an example corpus document\"],\n",
    "]\n",
    "\n",
    "embeddings = predictor.predict(sentences)\n",
    "\n",
    "print(f\"Embedding dimensions: {len(embeddings[0])}\")\n",
    "# print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b17d1e-2181-49b4-b8ec-f1d9b78f98e1",
   "metadata": {},
   "source": [
    "### Verify Logs emitted by the endpoint in CloudWatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ee94c-19a3-42ce-9d16-6cf9aceedf13",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "logs_client = boto3.client(\"logs\")\n",
    "\n",
    "# Get the current time and calculate the timestamp for 10 minutes ago\n",
    "current_time = int(time.time() * 1000)\n",
    "ten_minutes_ago = current_time - (10 * 60 * 1000)\n",
    "\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "\n",
    "# Get the log streams within the log group\n",
    "log_streams_response = logs_client.describe_log_streams(logGroupName=log_group_name)\n",
    "\n",
    "# Iterate through each log stream and print the logs\n",
    "for log_stream in log_streams_response[\"logStreams\"]:\n",
    "    log_stream_name = log_stream[\"logStreamName\"]\n",
    "\n",
    "    # Get the logs for the specific log stream\n",
    "    log_events_response = logs_client.get_log_events(\n",
    "        logGroupName=log_group_name,\n",
    "        logStreamName=log_stream_name,\n",
    "        startTime=ten_minutes_ago,\n",
    "        endTime=current_time,\n",
    "    )\n",
    "\n",
    "    # Print the logs\n",
    "    for event in log_events_response[\"events\"]:\n",
    "        print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c0fb5-96db-4b47-9dd5-66bfd9c822d6",
   "metadata": {},
   "source": [
    "### Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0dedf-fbbc-48a1-ae91-c64cdf59a588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictor.delete_model()\n",
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
